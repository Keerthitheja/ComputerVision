{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import deque\n",
    "from imutils.video import VideoStream\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV Version: 4.1.0\n",
      "ImUtils Version: 0.5.3\n",
      "Numpy Version: 1.18.1\n"
     ]
    }
   ],
   "source": [
    "print(\"OpenCV Version:\", cv2.__version__)\n",
    "print(\"ImUtils Version:\", imutils.__version__)\n",
    "print(\"Numpy Version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_width, screen_height = 1920, 1080\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Image Frames Directory doesn't exists!! Creating a new directory\n",
      "Processed Frames Directory doesn't exists!! Creating a new directory\n"
     ]
    }
   ],
   "source": [
    "ImageDirectory = \"ImageFrames/\"\n",
    "ProcessedFrameDirectory = \"ProcessedFrames/\"\n",
    "if not os.path.isdir(ImageDirectory):\n",
    "    print(\"Original Image Frames Directory doesn't exists!! Creating a new directory\")\n",
    "    os.mkdir(ImageDirectory)\n",
    "else:\n",
    "    print(\"Image Directory Exists !!!\")\n",
    "    \n",
    "if not os.path.isdir(ProcessedFrameDirectory):\n",
    "    print(\"Processed Frames Directory doesn't exists!! Creating a new directory\")\n",
    "    os.mkdir(ProcessedFrameDirectory)\n",
    "else:\n",
    "    print(\"Processed Image Directory Exists !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot read the Input Video\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"SourceVideoFile/InputVideoFile.mp4\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "while(True):\n",
    "    ret, vid_frm = capture.read()\n",
    "    \n",
    "    if( ret== True):\n",
    "        vid_frm = cv2.resize(vid_frm,(width, height))\n",
    "        \n",
    "        cv2.imwrite(ImageDirectory+\"Frame%d.jpg\" %counter, vid_frm )\n",
    "        counter += 1\n",
    "    else:\n",
    "        print(\"Cannot read the Input Video anymore !!!\")\n",
    "        print(\"Exiting Video Capture\")\n",
    "        break\n",
    "        \n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video FPS:  24.0\n",
      "Output folder to write the video file exists !!!\n"
     ]
    }
   ],
   "source": [
    "capture = cv2.VideoCapture(\"SourceVideoFile/InputVideoFile.mp4\")\n",
    "capture.set(3,320)\n",
    "capture.set(4,240)\n",
    "\n",
    "(major_version, minor_version, subminor_version) = (cv2.__version__).split('.')\n",
    "if (int(major_version) < 3):\n",
    "    video_fps = capture.get(cv2.cv.CV_CAP_PROP_FPS)\n",
    "else:\n",
    "    video_fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "video_fps = float(video_fps)\n",
    "\n",
    "print(\"video FPS: \", video_fps)\n",
    "if (video_fps == 0):\n",
    "    print(\"Please check the input video file !!!\")\n",
    "else:\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    #height, width = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height , width = screen_height/4, screen_width/4\n",
    "    height = int(height)\n",
    "    width = int(width)\n",
    "    zeros = np.zeros((height,width), dtype='uint8')\n",
    "\n",
    "    if not os.path.isdir(\"OutputVideoFile\"):\n",
    "        os.mkdir(\"OutputVideoFile\")\n",
    "    else:\n",
    "        print(\"Output folder to write the video file exists !!!\")\n",
    "\n",
    "    output = cv2.VideoWriter('OutputVideoFile/output.mp4', fourcc, video_fps, (width*2,height*2))\n",
    "    \n",
    "zero_20_secs_frames = (video_fps) * 21\n",
    "twenty_40_secs_frames = (video_fps) * 42\n",
    "forty_60_secs_frames = (video_fps) * 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height of Frame:  270\n",
      "Width of Frame:  480\n"
     ]
    }
   ],
   "source": [
    "print(\"Height of Frame: \", height)\n",
    "print(\"Width of Frame: \", width)\n",
    "frame_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_subtitles(image,text, color, org):\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    fontScale = 0.9\n",
    "    thickness = 1\n",
    "   # Using cv2.putText() \n",
    "    image_out = cv2.putText(image, text, org, font, fontScale, color, thickness, cv2.LINE_AA, False)\n",
    "    return image_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_blur_filter(video_frame,kernel_height,kernel_width):\n",
    "    blurred_frame = cv2.GaussianBlur(video_frame,(kernel_height,kernel_width),cv2.BORDER_DEFAULT)\n",
    "    blurred_frame = write_subtitles(blurred_frame, \n",
    "                                    \"Gaussian filter with kernel size {}, {}\".\n",
    "                                    format(kernel_height, kernel_width),(255, 255, 255),(20, 250))\n",
    "    return blurred_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_lateral_filter(video_frame,diameter,signalColor,sigmaSpace):\n",
    "    filtered_frame = cv2.bilateralFilter(video_frame,diameter,signalColor,sigmaSpace)\n",
    "    filtered_frame = write_subtitles(filtered_frame, \n",
    "                                    \"Bi-lateral filter with Diameter={}\".\n",
    "                                     format(diameter), (255, 255, 255),(20, 200) )\n",
    "    filtered_frame = write_subtitles(filtered_frame, \"SigColor={} and SigSpace={}\".\n",
    "                                     format(signalColor, sigmaSpace), (255, 255, 255),(20, 250) )\n",
    "    return filtered_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_filter(frame,median_value):\n",
    "    filtered_med_frame = cv2.medianBlur(frame, 5)\n",
    "    filtered_med_frame = write_subtitles(filtered_med_frame, \n",
    "                                    \"Median filter with Median value={}\".format(median_value),\n",
    "                                        (255, 255, 255), (20, 250))\n",
    "    return filtered_med_frame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabbing_object(video_frame):\n",
    "    rgb_frame = cv2.cvtColor(video_frame, cv2.COLOR_BGR2RGB)\n",
    "    hsv_frame = cv2.cvtColor(video_frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_blue = np.array([95,110,20])\n",
    "    upper_blue = np.array([105,255,255])\n",
    "    \n",
    "    mask = cv2.inRange(hsv_frame, lower_blue, upper_blue)\n",
    "    # Bitwise-AND mask and original image\n",
    "    resulting_image = cv2.bitwise_and(video_frame,video_frame, mask= mask)\n",
    "    \n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    opening_morph_frame = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    closing_morph_frame = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    #cv2.imshow(\"masked image\", mask)\n",
    "    #cv2.imshow(\"Unmasked Image\", resulting_image)\n",
    "    masked_image = cv2.merge([mask, mask, mask])\n",
    "    open_morph_img = cv2.merge([zeros, opening_morph_frame, zeros])\n",
    "    close_morph_img = cv2.merge([zeros, zeros, closing_morph_frame]) \n",
    "    \n",
    "    open_morph_img = write_subtitles(open_morph_img, \"Opening Morphological Transform :\",\n",
    "                                        (75, 155, 200), (20, 200))\n",
    "    \n",
    "    close_morph_img = write_subtitles(close_morph_img, \n",
    "                                    \"Closed Morphological Transform\",\n",
    "                                        (75, 155, 200), (20, 200))\n",
    "    close_morph_img = write_subtitles(close_morph_img, \n",
    "                                    \"Dilation followed by Erosion\",\n",
    "                                        (75, 155, 200), (20, 250))\n",
    "    open_morph_img = write_subtitles(open_morph_img, \"Erosion followed by dilation\",\n",
    "                                        (75, 155, 200),(20, 250))\n",
    "    \n",
    "    masked_image = write_subtitles(masked_image, \n",
    "                                    \"Binary Image with only Object\",\n",
    "                                        (75, 155, 200),(20, 250))\n",
    "    \n",
    "    return masked_image, open_morph_img, close_morph_img, mask, opening_morph_frame, closing_morph_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_20_secs_basic_image_processing(frame_count, raw_image):\n",
    "    four_secs_frames = video_fps*5\n",
    "    twelve_secs_frames = video_fps*13\n",
    "    twenty_secs_frames = video_fps*21\n",
    "    output_frame = np.zeros((height*2,width*2,3),dtype='uint8')\n",
    "        \n",
    "    if(frame_count <= twelve_secs_frames):\n",
    "        if(frame_count <= four_secs_frames*2):\n",
    "            \n",
    "            output_frame[0:height, width:width*2] = gaussian_blur_filter(raw_image,3,3)\n",
    "            output_frame[height:height*2, width:width*2] = bi_lateral_filter(raw_image,9,75,75)\n",
    "            output_frame[height:height*2,0:width] = median_filter(raw_image, 3)\n",
    "        else:\n",
    "        \n",
    "            output_frame[0:height, width:width*2] = gaussian_blur_filter(raw_image,5,5)\n",
    "            output_frame[height:height*2, width:width*2] = bi_lateral_filter(raw_image,15,75,75)\n",
    "            output_frame[height:height*2,0:width] = median_filter(raw_image, 5)\n",
    "        #cv2.imshow(\"Video Frame\", raw_image)\n",
    "        if(frame_count >= 2*video_fps and frame_count <= 6*video_fps):\n",
    "            gray_frame = cv2.cvtColor(raw_image.copy(), cv2.COLOR_BGR2GRAY)\n",
    "            gray_frame = cv2.resize(gray_frame,(width,height))\n",
    "            gray_frame_merge = cv2.merge([gray_frame, gray_frame, gray_frame])\n",
    "            Gray_Img = write_subtitles(gray_frame_merge, \"Gray Sclae Image frame\", (255,255,255),(20, 250))\n",
    "            output_frame[0:height, 0:width] = Gray_Img\n",
    "        else:\n",
    "            Original_Img = write_subtitles(raw_image, \"Original Image frame\", (255,255,255),(20, 250))\n",
    "            output_frame[0:height, 0:width] = Original_Img\n",
    "        #cv2.imshow(\"Original Image\", raw_image)\n",
    "    \n",
    "    else:\n",
    "        video_frame_text = write_subtitles(raw_image, \"Original Image frame\",(255,255,255),(20, 250))\n",
    "        output_frame[0:height, 0:width] = video_frame_text\n",
    "        \n",
    "        masked_image, open_morph_img, close_morph_img, _ ,_,_ = grabbing_object(raw_image)\n",
    "        \n",
    "        output_frame[0:height, width:width*2] = masked_image\n",
    "        output_frame[height:height*2, width:width*2] = open_morph_img\n",
    "        output_frame[height:height*2,0:width] = close_morph_img\n",
    "        \n",
    "    return output_frame\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_edge_color(sobelX, sobelY):\n",
    "    orientation = cv2.phase(sobelX, sobelY, angleInDegrees=True)\n",
    "    magnitude = cv2.magnitude(sobelX, sobelY)\n",
    "\n",
    "    # thresholding of the magnitude values, play with the thresh value adjust it too your liking\n",
    "    threshold = 100\n",
    "    _, mask = cv2.threshold(magnitude, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    red = np.array([0, 0, 255])\n",
    "    cyan = np.array([255, 255, 0])\n",
    "    green = np.array([0, 255, 0])\n",
    "    yellow = np.array([0, 255, 255])\n",
    "\n",
    "    image_map = np.zeros((orientation.shape[0], orientation.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    image_map[ (mask == 255) & (orientation < 90) ] = red\n",
    "    image_map[(mask == 255) & (orientation > 90) & (orientation < 180)] = cyan\n",
    "    image_map[(mask == 255) & (orientation > 180) & (orientation < 270)] = green\n",
    "    image_map[(mask == 255) & (orientation > 270)] = yellow\n",
    "    \n",
    "    return image_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sobel_edge_detection(input_frame,ksize):\n",
    "    gray_image = cv2.cvtColor(input_frame, cv2.COLOR_BGR2GRAY)\n",
    "    # remove noise\n",
    "    #noise_free_img = cv2.GaussianBlur(gray_image,(3,3),0)\n",
    "    #sobelx = cv2.Sobel(noise_free_img,cv2.CV_64F,1,0,ksize)  # x\n",
    "    #sobely = cv2.Sobel(noise_free_img,cv2.CV_64F,0,1,ksize)  # y\n",
    "    #cv2.imshow(\"Sobel X\", sobelx)\n",
    "    #cv2.imshow(\"Sobel Y\", sobely)\n",
    "    sobelx = cv2.Sobel(gray_image,cv2.CV_32F,1,0,ksize)  # x\n",
    "    sobely = cv2.Sobel(gray_image,cv2.CV_32F,0,1,ksize)  # y\n",
    "    \n",
    "    colored_edge_image = detect_edge_color(sobelx, sobely)\n",
    "        \n",
    "    sobelx = cv2.cvtColor(sobelx,cv2.COLOR_GRAY2RGB)\n",
    "    sobely = cv2.cvtColor(sobely,cv2.COLOR_GRAY2RGB)\n",
    "    #sobelx_img_frm = cv2.merge([sobelx_img, sobelx_img, sobelx_img])\n",
    "    #sobely_img_frm = cv2.merge([sobely_img, sobely_img, sobely_img])\n",
    "    \n",
    "    sobelx = write_subtitles(sobelx, \"Sobel filter Vertical gradient with Kernel Size = {} \".format(ksize),\n",
    "                                        (0, 0, 255),(20, 20))\n",
    "    \n",
    "    sobely = write_subtitles(sobely, \"Sobel filter Horizontal gradient with Kernel Size = {} \".format(ksize),\n",
    "                                        (0, 0, 255),(20, 20))\n",
    "    \n",
    "    colored_edge_image = write_subtitles(colored_edge_image, \n",
    "                                         \"Edge Detected image with Sobel Kernel size={}\".format(ksize),\n",
    "                                        (255, 255, 255),(20, 20))\n",
    "    \n",
    "    return sobelx , sobely, colored_edge_image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_circle_hough(input_frame, dp, minDist, minRad, maxRad):\n",
    "    gray_img_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2GRAY)\n",
    "    output_img = input_frame.copy()\n",
    "    gray_blurred = cv2.medianBlur(gray_img_frame,5)\n",
    "    \n",
    "    rows = gray_blurred.shape[0]\n",
    "    \n",
    "    circles = cv2.HoughCircles(gray_blurred, cv2.HOUGH_GRADIENT, dp, minDist, param1=100, param2=30,minRadius=minRad,maxRadius=maxRad)\n",
    "            \n",
    "    #circles = cv2.HoughCircles(gray_blurred, cv2.HOUGH_GRADIENT, 0.5, rows / 8, param1=100, param2=30,\n",
    "                               # minRadius=1, maxRadius=100=)\n",
    "    \n",
    "    \n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        for i in circles[0, :]:\n",
    "            center = (i[0], i[1])\n",
    "            # circle center\n",
    "            #cv2.circle(input_frame, center, 1, (0, 100, 100), 3)\n",
    "            # circle outline\n",
    "            radius = i[2]\n",
    "            cv2.circle(output_img, center, radius, (255, 0, 255), 3)\n",
    "    \n",
    "    out_image_text = write_subtitles(output_img, \"Circle Detection: Hough transform with dp= {},\"\n",
    "                                    .format(dp), (255,255,255), (20, 200))\n",
    "    out_image_text = write_subtitles(out_image_text, \"minDist={}, minRadius={}, maxRadius={}\"\n",
    "                                    .format(minDist,minRad,maxRad), (255,255,255), (20, 250))\n",
    "    \n",
    "    return out_image_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_localization_and_contours(in_img):\n",
    "    image = in_img.copy()\n",
    "    _,_,_,masked_imaged,_,_ = grabbing_object(image)\n",
    "    ret, threshed_img = cv2.threshold(masked_imaged, 40, 255, cv2.THRESH_BINARY)\n",
    "    output = cv2.bitwise_and(in_img, in_img, mask=masked_imaged)\n",
    "    \n",
    "    # find contours and get the external one\n",
    "    contours,hierarchy = cv2.findContours(threshed_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    if len(contours) != 0:\n",
    "        \n",
    "        # draw in blue the contours that were founded\n",
    "        cv2.drawContours(output, contours, -1, 255, 3)\n",
    "        \n",
    "        # find the biggest countour (c) by the area\n",
    "        biggest_contour = max(contours, key = cv2.contourArea)\n",
    "    \n",
    "        x,y,w,h = cv2.boundingRect(biggest_contour)\n",
    "        image_template = image[y:y+h, x:x+w]\n",
    "        # draw the biggest contour (c) in green\n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "        cv2.imwrite('object_template.jpg', image_template)\n",
    "   \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_template_matching(input_img):\n",
    "    img = input_img.copy()\n",
    "    img2 = cv2.cvtColor(img.copy() , cv2.COLOR_BGR2GRAY)\n",
    "    template = cv2.imread('object_template.jpg',0)\n",
    "    w, h = template.shape[::-1]\n",
    "    \n",
    "    # All the 6 methods for comparison in a list\n",
    "    #methods = ['cv2.TM_CCOEFF', 'cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR' , 'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED']\n",
    "    meth = 'cv2.TM_CCOEFF_NORMED'\n",
    "    \n",
    "    method = eval(meth)\n",
    "    img = img2.copy()\n",
    "        # Apply template Matching\n",
    "    res = cv2.matchTemplate(img,template,method)\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "\n",
    "        # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum\n",
    "    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "        top_left = min_loc\n",
    "    else:\n",
    "        top_left = max_loc\n",
    "    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "\n",
    "    cv2.rectangle(img,top_left, bottom_right, 255, 2)\n",
    "        #cv2.imshow(\"MatchingResult\", res)\n",
    "        #cv2.imshow(\"img\",img)\n",
    "        \n",
    "    return res, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twenty_40_secs_object_detection(frame_count, input_frame, end_frame_cnt):\n",
    "    input_img_frame = input_frame.copy()\n",
    "    output_frame = np.zeros((height*2,width*2,3),dtype='uint8')\n",
    "    zeros_fcn = np.zeros((height,width), dtype='uint8')\n",
    "    two_secs_frames = video_fps*2+end_frame_cnt\n",
    "    five_secs_frames = video_fps*5+end_frame_cnt\n",
    "    ten_secs_frames = video_fps*10+end_frame_cnt\n",
    "    if((frame_count <= five_secs_frames)):\n",
    "        if(frame_count <= two_secs_frames):\n",
    "            sobelx_img, sobely_img, color_edge_img = sobel_edge_detection(input_frame,3)\n",
    "        else:\n",
    "            sobelx_img, sobely_img, color_edge_img = sobel_edge_detection(input_frame, 5)\n",
    "            \n",
    "        output_frame[0:height, width:width*2] = sobely_img\n",
    "        output_frame[height:height*2, width:width*2] = sobelx_img\n",
    "        output_frame[height:height*2,0:width] = color_edge_img\n",
    "        video_frame_text = write_subtitles(input_img_frame, \"Original Image frame\",(255,255,255), (20, 250))\n",
    "        output_frame[0:height, 0:width] = video_frame_text\n",
    "\n",
    "    elif((frame_count > five_secs_frames) and (frame_count <= (ten_secs_frames+(4*video_fps)))):\n",
    "        rows = input_frame.shape[0]\n",
    "        img_c1 = detect_circle_hough(input_frame, 0.5, rows/2, 1, 100)\n",
    "        img_c2 = detect_circle_hough(input_frame, 0.9, rows/4, 1, 120)\n",
    "        img_c3 = detect_circle_hough(input_frame, 1.1, rows/8, 1, 150)\n",
    "        \n",
    "        output_frame[0:height, width:width*2] = img_c1\n",
    "        output_frame[height:height*2, width:width*2] = img_c2\n",
    "        output_frame[height:height*2,0:width] = img_c3\n",
    "        video_frame_text = write_subtitles(input_img_frame, \"Original Image frame\",(255,255,255),(20, 250))\n",
    "        output_frame[0:height, 0:width] = video_frame_text\n",
    "    else:\n",
    "        if(frame_count <= (end_frame_cnt+(17*video_fps))):\n",
    "            image_with_obj = object_localization_and_contours(input_frame)\n",
    "            input_img_frame = cv2.resize(input_img_frame, (width, height*2))\n",
    "            image_with_obj = cv2.resize(image_with_obj,(width, height*2))\n",
    "            image_with_obj_text = write_subtitles(image_with_obj,\"Object Detection and Template Extraction\", \n",
    "                                                 (255,255,255), (20,250))\n",
    "            output_frame[0:height*2, width:width*2] = image_with_obj_text\n",
    "            video_frame_text = write_subtitles(input_img_frame, \"Original Image frame\",(255,255,255),(20, 250))\n",
    "            output_frame[0:height*2, 0:width] = video_frame_text\n",
    "        else:\n",
    "            print(\"In template matching !!!\")\n",
    "            temp_match_img = cv2.imread(\"upload.png\")\n",
    "            temp_match_img_text = write_subtitles(temp_match_img,\"Object Matched Template\", \n",
    "                                                 (255,255,255), (20,250))\n",
    "            \n",
    "            matching_img, gray_image = object_template_matching(input_frame)\n",
    "            matching_img = cv2.resize(matching_img,(width,height))\n",
    "            gray_image = cv2.resize(gray_image, (width, height))\n",
    "            input_img_frame = cv2.resize(input_img_frame, (width, height*2))\n",
    "            \n",
    "            matching_img_final = cv2.merge([matching_img, matching_img, matching_img])\n",
    "            gray_image = cv2.merge([gray_image, gray_image, gray_image])\n",
    "            \n",
    "            gray_image_text = write_subtitles(gray_image,\"Detected object from template\",(255,255,255),\n",
    "                                             (20,250))\n",
    "            #cv2.imshow(\"Temp Match\", matching_img_final)\n",
    "            #cv2.imwrite(\"template_match.jpg\",matching_img_final)\n",
    "            #temp_img = cv2.imread(\"template_match.jpg\")\n",
    "            temp_match_img_text = cv2.resize(temp_match_img_text,(width,height))\n",
    "            output_frame[0:height, width:width*2] = temp_match_img_text\n",
    "            #cv2.imshow(\"Matching Image\", matching_img_final)\n",
    "            #output_frame[0:height, width:width*2] = matching_img\n",
    "            output_frame[height:height*2, width:width*2] = gray_image\n",
    "            video_frame_text = write_subtitles(input_img_frame, \"Original Image frame\",(255,255,255),(20, 250))\n",
    "            output_frame[0:height*2, 0:width] = video_frame_text\n",
    "    \n",
    "    #cv2.imshow(\"Original Image\", raw_image)   \n",
    "    #output_frame = cv2.resize(video_frame,(width*2, height*2))\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_object_invisbile(input_image, background):\n",
    "    img = input_image.copy()\n",
    "    _,_,_,masked_imaged,_,_ = grabbing_object(img)\n",
    "    mask1 = cv2.morphologyEx(masked_imaged, cv2.MORPH_OPEN, np.ones((3,3),np.uint8),iterations=2)\n",
    "    mask1 = cv2.dilate(mask1,np.ones((3,3),np.uint8),iterations = 1)\n",
    "    mask2 = cv2.bitwise_not(mask1)\n",
    "\n",
    "    # Generating the final output\n",
    "    res1 = cv2.bitwise_and(background,background,mask=mask1)\n",
    "    res2 = cv2.bitwise_and(img,img,mask=mask2)\n",
    "    final_output = cv2.addWeighted(res1,1,res2,1,0)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_obj_color(input_image_frame, color):\n",
    "    image = input_image_frame.copy()\n",
    "    hsv=cv2.cvtColor(image,cv2.COLOR_BGR2HSV)\n",
    "   \n",
    "    lower_blue = np.array([95,110,20])\n",
    "    upper_blue = np.array([105,255,255])\n",
    "\n",
    "    # Mask image to only select browns\n",
    "    mask=cv2.inRange(hsv,lower_blue,upper_blue)\n",
    "    image[mask>0] = color\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forty_60_secs_carte_blanche(frame_count, video_frame, end_frames_count):\n",
    "    output_frame = np.zeros((height*2,width*2,3),dtype='uint8')\n",
    "    input_img_frame = video_frame.copy()\n",
    "    if(frame_count <= 1440):\n",
    "        background1 = cv2.imread(ImageDirectory+\"Frame1056.jpg\")\n",
    "        background2 = cv2.imread(ImageDirectory+\"Frame1215.jpg\")\n",
    "        background3 = cv2.imread(ImageDirectory+\"Frame1312.png\")\n",
    "        if(frame_count > 1054 and frame_count < 1146):\n",
    "            invisible_object = make_object_invisbile(input_img_frame, background1)\n",
    "            color_changed_object = change_obj_color(input_img_frame, (0,128,255))\n",
    "        elif(frame_count > 1184 and frame_count < 1261):\n",
    "            invisible_object = make_object_invisbile(input_img_frame, background2)\n",
    "            color_changed_object = change_obj_color(input_img_frame,(78,243,232))\n",
    "        elif((frame_count > 1261 and frame_count <1343) or (frame_count > 1374 and frame_count < 1399)):\n",
    "            invisible_object = make_object_invisbile(input_img_frame, background3)\n",
    "            color_changed_object = change_obj_color(input_img_frame, (158,100,169))\n",
    "        else:\n",
    "            invisible_object = input_img_frame.copy()\n",
    "            color_changed_object = input_img_frame.copy()\n",
    "        \n",
    "        tracked_obj = object_localization_and_contours(input_img_frame)\n",
    "        \n",
    "    else:\n",
    "        tracked_obj = video_frame.copy()\n",
    "        invisible_object = video_frame.copy()\n",
    "        color_changed_object = video_frame.copy()\n",
    "     \n",
    "    invisible_object_text = write_subtitles(invisible_object, \"Carte Blanche: Invisble Object\", (255,255,255),\n",
    "                                           (20,250))\n",
    "    tracked_obj_text = write_subtitles(tracked_obj, \"Carte Blanche: Object Tracking\", (255,255,255),\n",
    "                                           (20,250))\n",
    "    color_changed_object_text = write_subtitles(color_changed_object, \"Carte Blanche: Object Changing Colors\", (255,255,255),\n",
    "                                           (20,250))\n",
    "    output_frame[0:height, width:width*2] = invisible_object_text\n",
    "    output_frame[height:height*2, width:width*2] = tracked_obj_text\n",
    "    output_frame[height:height*2,0:width] = color_changed_object_text\n",
    "    #cv2.imshow(\"Video Frame\", raw_image)\n",
    "    Original_Img = write_subtitles(input_img_frame, \"Original Image frame\", (255,255,255),(20, 250))\n",
    "    output_frame[0:height, 0:width] = Original_Img\n",
    "    \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    ret_value, video_frame = capture.read()\n",
    "    \n",
    "    if( ret_value== True):\n",
    "        video_frame = cv2.resize(video_frame,(width, height))\n",
    "        \n",
    "        if (frame_count <= zero_20_secs_frames):\n",
    "            processed_frame = first_20_secs_basic_image_processing(frame_count,video_frame)\n",
    "            end_frames = frame_count\n",
    "        \n",
    "        elif ((frame_count > zero_20_secs_frames) and (frame_count <= twenty_40_secs_frames)):\n",
    "            processed_frame = twenty_40_secs_object_detection(frame_count, video_frame,end_frames)\n",
    "            end_frames_40 = frame_count\n",
    "        \n",
    "        else:\n",
    "            processed_frame = forty_60_secs_carte_blanche(frame_count, video_frame, end_frames)\n",
    "         \n",
    "        cv2.imwrite(ProcessedFrameDirectory+\"ProcessedFrame%d.jpg\" %frame_count, processed_frame) \n",
    "        cv2.imshow('OutputVideo', processed_frame)\n",
    "        if(frame_count <= 1440):\n",
    "            output.write(processed_frame)\n",
    "        c = cv2.waitKey(25)\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        if c & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "#print(\"End Frames 20:\", end_frames)\n",
    "#print(\"End Frames 40: \", end_frames_40)\n",
    "#print(\"Original End Frames 40:\", twenty_40_secs_frames)\n",
    "#print(\"Total Frame Counts: \", frame_count)\n",
    "capture.release()\n",
    "output.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-CV-GPU",
   "language": "python",
   "name": "tf-cv-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
